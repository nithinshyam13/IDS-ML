{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "EeBx2V4PkEG2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "ddS2IsnBkEG4"
      },
      "source": [
        "### 1. Load and Visualize Data\n",
        "\n",
        "Let's start by loading your entire dataset or a sample if it is a larger dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWNCtI02kND0",
        "outputId": "d65a7c66-fed3-4f60-f017-8a7dcb358f01"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "jFMT3P1bkEG5"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "# read dataset\n",
        "# data_file_name = \"./data/data.csv\"\n",
        "data_file_name = \"/content/drive/My Drive/CSC542 - F3/data/data.csv\"\n",
        "data_file_handlle = pd.read_csv(data_file_name)\n",
        "\n",
        "# define rows and coloums\n",
        "fields_x = [\n",
        "        'flow_duration', 'Header_Length', 'Protocol Type', 'Duration',\n",
        "        'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number',\n",
        "        'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
        "        'ece_flag_number', 'cwr_flag_number', 'ack_count',\n",
        "        'syn_count', 'fin_count', 'urg_count', 'rst_count',\n",
        "        'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
        "        'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min',\n",
        "        'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue',\n",
        "        'Radius', 'Covariance', 'Variance', 'Weight',\n",
        "]\n",
        "\n",
        "fields_y = ['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "gKKMmLq6kEG6"
      },
      "outputs": [],
      "source": [
        "# separate inputs and labels\n",
        "data_x = data_file_handlle[fields_x]\n",
        "data_y = data_file_handlle[fields_y]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "xU0GY-bekEG6"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "def calcMaxMinMean(data):\n",
        "    max_values = data.max(axis=0)\n",
        "    min_values = data.min(axis=0)\n",
        "    mean_values = data.mean(axis=0)\n",
        "\n",
        "    return (max_values, min_values, mean_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "6Ojd9LqzkEG6"
      },
      "outputs": [],
      "source": [
        "# Precomputing values of min, max, mean and standard deviation for the columns\n",
        "max_map = {}\n",
        "min_map = {}\n",
        "mean_map = {}\n",
        "std_map = {}\n",
        "\n",
        "for col_name in data_x.columns:\n",
        "    column = data_x[col_name]\n",
        "\n",
        "    max_map[col_name] = column.max()\n",
        "    min_map[col_name] = column.min()\n",
        "    mean_map[col_name] = column.mean()\n",
        "    std_map[col_name] = column.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "PhCbzp6EkEG7"
      },
      "outputs": [],
      "source": [
        "# Using Minimum and Maximum values of the columns to normalize the data\n",
        "\n",
        "# As there are columns with same min and max values, leading to a NaN value for the normalization, we do not perform normalization on those columns\n",
        "\n",
        "minmax_normalized_data = pd.DataFrame()\n",
        "\n",
        "for col_name in data_x.columns:\n",
        "    denominator = (max_map[col_name] - min_map[col_name])\n",
        "    if denominator != 0:\n",
        "        numerator = data_x[col_name] - min_map[col_name]\n",
        "        minmax_normalized_data[col_name] = numerator / denominator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "jFY9kcWCkEG7"
      },
      "outputs": [],
      "source": [
        "# Using Mean and Standard Deviation values of the columns to normalize the data\n",
        "\n",
        "# As there are columns with zero standard deviation, leading to a NaN value for the normalization, we do not perform normalization on those columns\n",
        "\n",
        "meanstd_normalized_data = pd.DataFrame()\n",
        "\n",
        "for col_name in data_x.columns:\n",
        "    denominator = std_map[col_name]\n",
        "    if denominator != 0:\n",
        "        numerator = data_x[col_name] - mean_map[col_name]\n",
        "        meanstd_normalized_data[col_name] = numerator / denominator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "1oyzdK28kEG8"
      },
      "source": [
        "Next, provide some characteristics about your output (e.g., histograms, spatial distribution of bounding boxes in the case of object detection, etc.). In the case of cases such as tasks such as reinforcement learning, you can provide details about the desired output and rewards used. You can show some correlations between inputs and outputs if possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "Ajr5-krykEG9"
      },
      "outputs": [],
      "source": [
        "# Using Minimum and Maximum values of the columns to normalize the data\n",
        "\n",
        "# As there are columns with same min and max values, leading to a NaN value for the normalization, we do not perform normalization on those columns\n",
        "minmax_normalized_data = pd.DataFrame()\n",
        "\n",
        "for col_name in data_x.columns:\n",
        "    denominator = (max_map[col_name] - min_map[col_name])\n",
        "    if denominator != 0:\n",
        "        numerator = data_x[col_name] - min_map[col_name]\n",
        "        minmax_normalized_data[col_name] = numerator / denominator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "MpUEL764kEG9"
      },
      "outputs": [],
      "source": [
        "# Using Mean and Standard Deviation values of the columns to normalize the data\n",
        "\n",
        "# As there are columns with zero standard deviation, leading to a NaN value for the normalization, we do not perform normalization on those columns\n",
        "meanstd_normalized_data = pd.DataFrame()\n",
        "\n",
        "for col_name in data_x.columns:\n",
        "    denominator = std_map[col_name]\n",
        "    if denominator != 0:\n",
        "        numerator = data_x[col_name] - mean_map[col_name]\n",
        "        meanstd_normalized_data[col_name] = numerator / denominator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "1_c48dyxkEG9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "s_scaler = StandardScaler()\n",
        "\n",
        "# scale the nd array\n",
        "s_scaler.fit(data_x)\n",
        "data_x_1 = s_scaler.transform(data_x)\n",
        "\n",
        "# convert DF to NP array\n",
        "X = data_x_1\n",
        "Y = data_y.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "dwz8_y-9kEG-"
      },
      "outputs": [],
      "source": [
        "# splitting data into training and validation sets\n",
        "import random, math\n",
        "def loadFeatures(X, Y, p, shift=0):\n",
        "    # p is the percentage of data to be loaded into validation set\n",
        "\n",
        "    total_data_entries = len(X)\n",
        "\n",
        "    val_idx_list = list(np.round(np.linspace(0, total_data_entries - 1, math.floor(total_data_entries * (p/100)))).astype(int))\n",
        "    if shift > 0:\n",
        "        val_idx_list = [i+shift for i in val_idx_list if ((i+shift) < total_data_entries)]\n",
        "    train_idx_list = list(set(np.array(range(total_data_entries))).difference(val_idx_list))\n",
        "\n",
        "    # print(val_idx_list[:20])\n",
        "    # print(train_idx_list[:20])\n",
        "\n",
        "    return X[val_idx_list], X[train_idx_list], Y[val_idx_list], Y[train_idx_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "7WnbO0PhkEG-"
      },
      "outputs": [],
      "source": [
        "xVal, xTrain, yVal, yTrain = loadFeatures(X, Y, 15, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "vRvASM8LkEG-"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# This function produces a summary of performance metrics including a confusion matrix\n",
        "def summaryPerf(yTrain,yTrainHat,y,yHat,OH=False):\n",
        "    # Plotting confusion matrix for the non-training set:\n",
        "    # cm = metrics.confusion_matrix(y,yHat,normalize='true')\n",
        "    # disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=fields_y)\n",
        "    # disp.plot()\n",
        "\n",
        "    # Displaying metrics for training and non-training sets\n",
        "    if OH:\n",
        "        yTrain=np.argmax(yTrain,axis=1)\n",
        "        y = np.argmax(y,axis=1)\n",
        "    print('Training:  Acc = {:4.3f}'.format(metrics.accuracy_score(yTrain,yTrainHat)))\n",
        "    print('Non-Train: Acc = {:4.3f}'.format(metrics.accuracy_score(y,yHat)))\n",
        "    print('Training:  BalAcc = {:4.3f}'.format(metrics.balanced_accuracy_score(yTrain,yTrainHat,adjusted=True)))\n",
        "    print('Non-Train: BalAcc = {:4.3f}'.format(metrics.balanced_accuracy_score(y,yHat)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_wl9SnMkEG-"
      },
      "source": [
        "# RF todo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "lqU2ud-WkEG_"
      },
      "outputs": [],
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from cuml import RandomForestClassifier as RandomForestClassifierGPU\n",
        "# # Defining the model using default parameters\n",
        "# M_RF = RandomForestClassifier()\n",
        "\n",
        "# # Training the model\n",
        "# M_RF.fit(xTrain,yTrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpfwhlQQkEG_"
      },
      "source": [
        "# MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLrwaPqzkEG_",
        "outputId": "732c6a4f-b99d-45ee-90b5-c6d6c9492a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Setup device-agnostic code\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\" # NVIDIA GPU\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\" # Apple GPU\n",
        "else:\n",
        "    device = \"cpu\" # Defaults to CPU if NVIDIA GPU/Apple GPU aren't available\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "R48-y1LykEG_"
      },
      "outputs": [],
      "source": [
        "all_attacks_list = ['DDoS-RSTFINFlood', 'DoS-TCP_Flood', 'DDoS-ICMP_Flood', 'DoS-UDP_Flood', 'DoS-SYN_Flood', 'Mirai-greeth_flood', 'DDoS-SynonymousIP_Flood', 'Mirai-udpplain', 'DDoS-SYN_Flood', 'DDoS-PSHACK_Flood', 'DDoS-TCP_Flood', 'DDoS-UDP_Flood', 'BenignTraffic', 'MITM-ArpSpoofing', 'DDoS-ACK_Fragmentation', 'Mirai-greip_flood', 'DoS-HTTP_Flood', 'DDoS-ICMP_Fragmentation', 'Recon-PortScan', 'DNS_Spoofing', 'DDoS-UDP_Fragmentation', 'Recon-OSScan', 'XSS', 'DDoS-HTTP_Flood', 'Recon-HostDiscovery', 'CommandInjection', 'VulnerabilityScan', 'DDoS-SlowLoris', 'Backdoor_Malware', 'BrowserHijacking', 'DictionaryBruteForce', 'SqlInjection', 'Recon-PingSweep', 'Uploading_Attack']\n",
        "\n",
        "yLabels = sorted(all_attacks_list)\n",
        "yLabelsDict = {}\n",
        "\n",
        "for idx, l in enumerate(yLabels):\n",
        "    yLabelsDict[l] = idx\n",
        "\n",
        "yTrainInt = np.array([yLabelsDict[y[0]] for y in yTrain])\n",
        "yValInt = np.array([yLabelsDict[y[0]] for y in yVal])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def createOH(arr, searchInDict=True):\n",
        "    logit_len = len(all_attacks_list)\n",
        "\n",
        "    OHArr = []\n",
        "\n",
        "    for i in arr:\n",
        "        tmp = [float(0.0)] * logit_len\n",
        "        if searchInDict:\n",
        "            tmp[all_attacks_list.index(i[0])] = float(1.0)\n",
        "        else:\n",
        "            tmp[i] = float(1.0)\n",
        "        tmp = np.array(tmp)\n",
        "        OHArr.append(tmp)\n",
        "\n",
        "    return np.array(OHArr)\n"
      ],
      "metadata": {
        "id": "m6z2utTEwcTn"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "4YoNgQvqkEHA"
      },
      "outputs": [],
      "source": [
        "logit_len = len(all_attacks_list)\n",
        "\n",
        "yTrainOH = createOH(yTrain, True)\n",
        "yValOH = createOH(yVal, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "yeubtPO8kEHA"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "# Define the model architecture\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self,device,epochs,weights):\n",
        "        super(MLP, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(46, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 64),\n",
        "            nn.Dropout(0.05),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 30),\n",
        "            nn.Dropout(0.10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(30, 34),\n",
        "        ).to(device)\n",
        "\n",
        "        for layer in self.model:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                init.xavier_normal_(layer.weight)\n",
        "\n",
        "        self.device = device\n",
        "        self.epochs = epochs\n",
        "        self.loss_fn = nn.CrossEntropyLoss(weight = torch.tensor(weights))\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=2.5e-3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.model(x)\n",
        "        return logits\n",
        "\n",
        "    def fit(self,X,y):\n",
        "        X = torch.from_numpy(X).float()\n",
        "        X = X.to(device)\n",
        "        y = torch.from_numpy(y).float()\n",
        "        y = y.to(device)\n",
        "\n",
        "        for t in range(self.epochs):\n",
        "            pred = self.model(X)\n",
        "            loss = self.loss_fn(pred, y)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if t%100 == 99:\n",
        "                print(\"[Epoch {t:5d} of {epochs}] loss: {loss:1.6f}\".format(\n",
        "                    t=t+1,epochs=self.epochs,loss=loss))\n",
        "\n",
        "    def predict(self,X):\n",
        "        X = torch.from_numpy(X).float()\n",
        "        X = X.to(self.device)\n",
        "\n",
        "        pred = self.model(X)\n",
        "        pred = pred.cpu().detach().numpy()\n",
        "        pred = np.argmax(pred,axis=1)\n",
        "\n",
        "        return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "QmqPo1hXkEHA"
      },
      "outputs": [],
      "source": [
        "#calc weights and set epoch\n",
        "\n",
        "unique, counts = np.unique(yTrain, return_counts=True)\n",
        "counts = dict(zip(unique, counts))\n",
        "total_count = yTrain.size\n",
        "weights_hp = []\n",
        "for i in counts.values():\n",
        "    tmp = 1 - (i / total_count)\n",
        "    weights_hp.append(float(tmp))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_hp = 2000"
      ],
      "metadata": {
        "id": "wsKdKyHFwWnX"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vBPOkgVkEHA",
        "outputId": "0766b953-5137-4092-8750-749fb7872b44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=46, out_features=100, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=100, out_features=64, bias=True)\n",
            "  (3): Dropout(p=0.05, inplace=False)\n",
            "  (4): ReLU()\n",
            "  (5): Linear(in_features=64, out_features=30, bias=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): ReLU()\n",
            "  (8): Linear(in_features=30, out_features=34, bias=True)\n",
            ")\n",
            "[Epoch   100 of 2000] loss: 0.491710\n",
            "[Epoch   200 of 2000] loss: 0.437271\n",
            "[Epoch   300 of 2000] loss: 0.406753\n",
            "[Epoch   400 of 2000] loss: 0.373629\n",
            "[Epoch   500 of 2000] loss: 0.319703\n",
            "[Epoch   600 of 2000] loss: 0.275308\n",
            "[Epoch   700 of 2000] loss: 0.279189\n",
            "[Epoch   800 of 2000] loss: 0.195223\n",
            "[Epoch   900 of 2000] loss: 0.147429\n",
            "[Epoch  1000 of 2000] loss: 0.233739\n",
            "[Epoch  1100 of 2000] loss: 0.118073\n",
            "[Epoch  1200 of 2000] loss: 0.098481\n",
            "[Epoch  1300 of 2000] loss: 0.085737\n",
            "[Epoch  1400 of 2000] loss: 0.078899\n",
            "[Epoch  1500 of 2000] loss: 0.073883\n",
            "[Epoch  1600 of 2000] loss: 0.069364\n",
            "[Epoch  1700 of 2000] loss: 0.067056\n",
            "[Epoch  1800 of 2000] loss: 0.064360\n",
            "[Epoch  1900 of 2000] loss: 0.063329\n",
            "[Epoch  2000 of 2000] loss: 0.060118\n"
          ]
        }
      ],
      "source": [
        "MLPModel = MLP(device,epochs_hp,weights_hp).to(device)\n",
        "print(MLPModel.model)\n",
        "MLPModel.fit(xTrain, yTrainOH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "oOwKTLXQkEHB"
      },
      "outputs": [],
      "source": [
        "yTrainHat = MLPModel.predict(xTrain)\n",
        "yValHat = MLPModel.predict(xVal)\n",
        "\n",
        "yTrainHatOH = createOH(yTrainHat, False)\n",
        "yValHatOH = createOH(yValHat, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUbh0DLEkEHB",
        "outputId": "aaaec871-0594-4ac1-c986-370a838fac88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for Validation:\n",
            "\n",
            "Training:  Acc = 0.982\n",
            "Non-Train: Acc = 0.982\n",
            "Training:  BalAcc = 0.620\n",
            "Non-Train: BalAcc = 0.626\n"
          ]
        }
      ],
      "source": [
        "print('Results for Validation:\\n')\n",
        "summaryPerf(yTrainOH,yTrainHat,yValOH,yValHat,True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf9iEJ6gkEHB"
      },
      "source": [
        "# LG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFV-7FGFkEHB"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model_lg = LogisticRegression(n_jobs=-1)\n",
        "\n",
        "model_lg.fit(xTrain, yTrain.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrslDzTIkEHB"
      },
      "outputs": [],
      "source": [
        "yTrainHat1 = model_lg.predict(xTrain)\n",
        "yValHat1 = model_lg.predict(xVal)\n",
        "\n",
        "print('Results for Validation:\\n')\n",
        "summaryPerf(yTrainOH,yTrainHat1,yValOH,yValHat1)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}